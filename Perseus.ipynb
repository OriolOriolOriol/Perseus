{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perseus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRqSPkrvjhF0AFl2zEvEww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OriolOriolOriol/Perseus/blob/main/Perseus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhmCxZB4ePqd"
      },
      "source": [
        "import numpy\r\n",
        "import sys\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.utils import np_utils\r\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw3cPnJIgI_U"
      },
      "source": [
        "Importato le librerie necessari per creare il modello.\r\n",
        "Ecco di seguito cosa fanno:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uubPzIo3gJPl"
      },
      "source": [
        "filename=\"/content/libro.txt\"\r\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\r\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y0hpbEBg8o1"
      },
      "source": [
        "Carichiamo  il libro  in memoria e convertire tutti i caratteri in minuscolo per ridurre il vocabolario che la rete deve imparare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wDlsNu2hClh",
        "outputId": "f06056dd-94fd-4563-f7c6-8bd33036b183"
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\r\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\r\n",
        "print(char_to_int)\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, ';': 20, '?': 21, '_': 22, 'a': 23, 'b': 24, 'c': 25, 'd': 26, 'e': 27, 'f': 28, 'g': 29, 'h': 30, 'i': 31, 'j': 32, 'l': 33, 'm': 34, 'n': 35, 'o': 36, 'p': 37, 'q': 38, 'r': 39, 's': 40, 't': 41, 'u': 42, 'v': 43, 'x': 44, 'y': 45, 'z': 46, '«': 47, '°': 48, '²': 49, '¹': 50, '»': 51, 'à': 52, 'è': 53, 'ê': 54, 'ì': 55, 'î': 56, 'ò': 57, 'ù': 58}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEa4ImMChGQV"
      },
      "source": [
        "Ora che il libro è caricato, dobbiamo preparare i dati per la modellazione dalla rete neurale. Non possiamo modellare i caratteri direttamente, dobbiamo invece convertire i caratteri in numeri interi.\r\n",
        "Prendi ogni carattere e le ordini in abse al suo codice ASCII. Successivamente si mappa ogni singolo carattere con un numero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTMjQ8smikcC",
        "outputId": "0d4ebffd-e01b-4638-f454-45ee2854f4eb"
      },
      "source": [
        "n_chars = len(raw_text)\r\n",
        "n_vocab = len(chars)\r\n",
        "print (\"Total Characters: \", n_chars)\r\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  120448\n",
            "Total Vocab:  59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jteVqfsmi7hk"
      },
      "source": [
        "Ora che il libro è stato caricato e la mappatura preparata, possiamo riassumere il set di dati."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9as4lF8i78z",
        "outputId": "8dd57028-0436-4d21-a819-6dc5e8eff7b1"
      },
      "source": [
        "lunghezza_sequenza= 100\r\n",
        "dataX=[]\r\n",
        "dataY=[]\r\n",
        "for i in range(0, n_chars - lunghezza_sequenza, 1):\r\n",
        "  seq_in= raw_text[i:i + lunghezza_sequenza]\r\n",
        "  seq_output= raw_text[i + lunghezza_sequenza]\r\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\r\n",
        "  dataY.append(char_to_int[seq_output])\r\n",
        "n_patterns= len(dataX)\r\n",
        "print(\"Totali Patterns: \", n_patterns)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Totali Patterns:  120348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSLNaZ0zi-0Q"
      },
      "source": [
        "Ora dobbiamo definire i dati di addestramento per la rete. C'è molta flessibilità nel modo in cui scegli di suddividere il testo ed esporlo alla rete durante l'allenamento.\r\n",
        "\r\n",
        "Possiamo dividere il testo del libro in sottosequenze di 100 caratteri. Ciascun training patterns è composto da 100 fasi temporali di un carattere X seguito da un output di carattere Y.\r\n",
        "\r\n",
        "\r\n",
        "Quando creiamo queste sequenze, facciamo scorrere questa finestra lungo l'intero libro un carattere alla volta, consentendo a ciascun carattere di apprendere dai 100 caratteri che lo hanno preceduto (tranne i primi 100 caratteri ovviamente).\r\n",
        "\r\n",
        "Una volta suddivisi convertiamo i caratteri in numeri interi usando la funzione creata in precedenza.\r\n",
        "\r\n",
        "L'esecuzione del codice fino a questo punto ci mostra che quando suddividiamo il set di dati in dati di addestramento affinché la rete apprenda che abbiamo poco meno di 150.000 modelli di addestramento. Questo ha senso escludendo i primi 100 caratteri, abbiamo un modello di addestramento per prevedere ciascuno dei caratteri rimanenti.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nafe39T6pc8k"
      },
      "source": [
        "#X\r\n",
        "\r\n",
        "# reshape X to be [samples, time steps, features]\r\n",
        "X = numpy.reshape(dataX, (n_patterns, lunghezza_sequenza, 1))\r\n",
        "# normalize\r\n",
        "X = X / float(n_vocab)\r\n",
        "\r\n",
        "#Y\r\n",
        "# one hot encode the output variable\r\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAPUnhEwpdIE"
      },
      "source": [
        "Ora che abbiamo preparato i nostri dati di allenamento, dobbiamo trasformarli in modo che siano adatti per l'uso con Keras.\r\n",
        "\r\n",
        "Per prima cosa dobbiamo trasformare l'elenco delle sequenze di input nella forma [campioni, fasi temporali, caratteristiche] attesi da una rete LSTM.\r\n",
        "\r\n",
        "Successivamente è necessario ridimensionare gli interi nell'intervallo da 0 a 1 per rendere i modelli più facili da apprendere dalla rete LSTM che utilizza la funzione di attivazione del sigmoide per impostazione predefinita.\r\n",
        "\r\n",
        "Infine, dobbiamo convertire i modelli di output (singoli caratteri convertiti in interi) in una codifica a caldo. Questo è così che possiamo configurare la rete per prevedere la probabilità di ciascuno dei 47 diversi caratteri nel vocabolario (una rappresentazione più semplice) piuttosto che cercare di costringerla a prevedere con precisione il carattere successivo. Ogni valore y viene convertito in un vettore sparse con una lunghezza di 47, pieno di zeri tranne con un 1 nella colonna per la lettera (numero intero) che il modello rappresenta.\r\n",
        "\r\n",
        "Ad esempio, quando \"n\" (valore intero 31) è una codifica a caldo, appare come segue:\r\n",
        "\r\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcNLQRRQqwxg",
        "outputId": "d642c1da-5705-48b7-e9ea-7be0905eec01"
      },
      "source": [
        "'''\r\n",
        "print(X.shape[1])\r\n",
        "print(X.shape[2])\r\n",
        "print(y.shape[1])#caratteri diversi\r\n",
        "'''\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(128, input_shape=(X.shape[1], X.shape[2])))\r\n",
        "model.add(LSTM(units=512))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "model.add(Dense(y.shape[1], activation='softmax')) #Softmax output shape\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "# define the checkpoint\r\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "model.summary()\r\n",
        "model.fit(X, y, epochs=30, batch_size=256, callbacks=callbacks_list)\r\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "471/471 [==============================] - 31s 64ms/step - loss: 2.9589\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.84996, saving model to weights-improvement-01-2.8500.hdf5\n",
            "Epoch 2/30\n",
            "471/471 [==============================] - 31s 66ms/step - loss: 2.6813\n",
            "\n",
            "Epoch 00002: loss improved from 2.84996 to 2.64876, saving model to weights-improvement-02-2.6488.hdf5\n",
            "Epoch 3/30\n",
            "471/471 [==============================] - 32s 67ms/step - loss: 2.5486\n",
            "\n",
            "Epoch 00003: loss improved from 2.64876 to 2.52525, saving model to weights-improvement-03-2.5253.hdf5\n",
            "Epoch 4/30\n",
            "471/471 [==============================] - 32s 68ms/step - loss: 2.4366\n",
            "\n",
            "Epoch 00004: loss improved from 2.52525 to 2.41183, saving model to weights-improvement-04-2.4118.hdf5\n",
            "Epoch 5/30\n",
            "471/471 [==============================] - 33s 69ms/step - loss: 2.3212\n",
            "\n",
            "Epoch 00005: loss improved from 2.41183 to 2.29616, saving model to weights-improvement-05-2.2962.hdf5\n",
            "Epoch 6/30\n",
            "471/471 [==============================] - 33s 70ms/step - loss: 2.2056\n",
            "\n",
            "Epoch 00006: loss improved from 2.29616 to 2.18261, saving model to weights-improvement-06-2.1826.hdf5\n",
            "Epoch 7/30\n",
            "471/471 [==============================] - 33s 70ms/step - loss: 2.0967\n",
            "\n",
            "Epoch 00007: loss improved from 2.18261 to 2.08297, saving model to weights-improvement-07-2.0830.hdf5\n",
            "Epoch 8/30\n",
            "471/471 [==============================] - 33s 70ms/step - loss: 1.9975\n",
            "\n",
            "Epoch 00008: loss improved from 2.08297 to 1.99385, saving model to weights-improvement-08-1.9938.hdf5\n",
            "Epoch 9/30\n",
            "471/471 [==============================] - 33s 71ms/step - loss: 1.9217\n",
            "\n",
            "Epoch 00009: loss improved from 1.99385 to 1.91651, saving model to weights-improvement-09-1.9165.hdf5\n",
            "Epoch 10/30\n",
            "471/471 [==============================] - 33s 71ms/step - loss: 1.8423\n",
            "\n",
            "Epoch 00010: loss improved from 1.91651 to 1.84468, saving model to weights-improvement-10-1.8447.hdf5\n",
            "Epoch 11/30\n",
            "471/471 [==============================] - 33s 71ms/step - loss: 1.7779\n",
            "\n",
            "Epoch 00011: loss improved from 1.84468 to 1.77882, saving model to weights-improvement-11-1.7788.hdf5\n",
            "Epoch 12/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.7144\n",
            "\n",
            "Epoch 00012: loss improved from 1.77882 to 1.71950, saving model to weights-improvement-12-1.7195.hdf5\n",
            "Epoch 13/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.6534\n",
            "\n",
            "Epoch 00013: loss improved from 1.71950 to 1.66151, saving model to weights-improvement-13-1.6615.hdf5\n",
            "Epoch 14/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.6031\n",
            "\n",
            "Epoch 00014: loss improved from 1.66151 to 1.60692, saving model to weights-improvement-14-1.6069.hdf5\n",
            "Epoch 15/30\n",
            "471/471 [==============================] - 34s 72ms/step - loss: 1.5448\n",
            "\n",
            "Epoch 00015: loss improved from 1.60692 to 1.55066, saving model to weights-improvement-15-1.5507.hdf5\n",
            "Epoch 16/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.4846\n",
            "\n",
            "Epoch 00016: loss improved from 1.55066 to 1.49822, saving model to weights-improvement-16-1.4982.hdf5\n",
            "Epoch 17/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.4380\n",
            "\n",
            "Epoch 00017: loss improved from 1.49822 to 1.45263, saving model to weights-improvement-17-1.4526.hdf5\n",
            "Epoch 18/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.3887\n",
            "\n",
            "Epoch 00018: loss improved from 1.45263 to 1.40391, saving model to weights-improvement-18-1.4039.hdf5\n",
            "Epoch 19/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.3399\n",
            "\n",
            "Epoch 00019: loss improved from 1.40391 to 1.35693, saving model to weights-improvement-19-1.3569.hdf5\n",
            "Epoch 20/30\n",
            "471/471 [==============================] - 34s 72ms/step - loss: 1.3006\n",
            "\n",
            "Epoch 00020: loss improved from 1.35693 to 1.31547, saving model to weights-improvement-20-1.3155.hdf5\n",
            "Epoch 21/30\n",
            "471/471 [==============================] - 34s 72ms/step - loss: 1.2562\n",
            "\n",
            "Epoch 00021: loss improved from 1.31547 to 1.27305, saving model to weights-improvement-21-1.2731.hdf5\n",
            "Epoch 22/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.2166\n",
            "\n",
            "Epoch 00022: loss improved from 1.27305 to 1.23565, saving model to weights-improvement-22-1.2357.hdf5\n",
            "Epoch 23/30\n",
            "471/471 [==============================] - 34s 72ms/step - loss: 1.1693\n",
            "\n",
            "Epoch 00023: loss improved from 1.23565 to 1.19467, saving model to weights-improvement-23-1.1947.hdf5\n",
            "Epoch 24/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.1424\n",
            "\n",
            "Epoch 00024: loss improved from 1.19467 to 1.16042, saving model to weights-improvement-24-1.1604.hdf5\n",
            "Epoch 25/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.1019\n",
            "\n",
            "Epoch 00025: loss improved from 1.16042 to 1.12199, saving model to weights-improvement-25-1.1220.hdf5\n",
            "Epoch 26/30\n",
            "471/471 [==============================] - 34s 72ms/step - loss: 1.0720\n",
            "\n",
            "Epoch 00026: loss improved from 1.12199 to 1.09188, saving model to weights-improvement-26-1.0919.hdf5\n",
            "Epoch 27/30\n",
            "471/471 [==============================] - 34s 72ms/step - loss: 1.0408\n",
            "\n",
            "Epoch 00027: loss improved from 1.09188 to 1.06156, saving model to weights-improvement-27-1.0616.hdf5\n",
            "Epoch 28/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 1.0075\n",
            "\n",
            "Epoch 00028: loss improved from 1.06156 to 1.03286, saving model to weights-improvement-28-1.0329.hdf5\n",
            "Epoch 29/30\n",
            "471/471 [==============================] - 34s 71ms/step - loss: 0.9877\n",
            "\n",
            "Epoch 00029: loss improved from 1.03286 to 0.99877, saving model to weights-improvement-29-0.9988.hdf5\n",
            "Epoch 30/30\n",
            "471/471 [==============================] - 34s 72ms/step - loss: 0.9620\n",
            "\n",
            "Epoch 00030: loss improved from 0.99877 to 0.97770, saving model to weights-improvement-30-0.9777.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f36816db320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHgFV3E7qxDg"
      },
      "source": [
        "Qui definiamo un singolo livello LSTM nascosto con 256 unità di memoria. La rete utilizza dropout con una probabilità di 20. Il livello di output è un livello Denso che utilizza la funzione di attivazione softmax per produrre una previsione di probabilità per ciascuno dei 47 caratteri compresi tra 0 e 1.\r\n",
        "\r\n",
        "Il problema è in realtà un problema di classificazione di un singolo carattere con 47 classi e come tale è definito come l'ottimizzazione della perdita di log (entropia incrociata), qui utilizzando l'algoritmo di ottimizzazione ADAM per la velocità.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Non siamo interessati al modello più accurato (accuratezza della classificazione) del set di dati di addestramento. Questo sarebbe un modello che prevede perfettamente ogni personaggio nel set di dati di addestramento. Ci interessa invece una generalizzazione del dataset che minimizzi la funzione di perdita scelta. Cerchiamo un equilibrio tra generalizzazione e overfitting ma a corto di memorizzazione.\r\n",
        "\r\n",
        "\r\n",
        "La rete è lenta da addestrare (circa 300 secondi per epoca su una GPU Nvidia K520). A causa della lentezza e dei nostri requisiti di ottimizzazione, utilizzeremo il checkpoint del modello per registrare tutti i pesi della rete da archiviare ogni volta che si osserva un miglioramento della perdita alla fine dell'epoca. Useremo il miglior set di pesi (perdita minima) per istanziare il nostro modello generativo nella sezione successiva."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XYdvu2o0MvU",
        "outputId": "35460985-c733-4364-d7ef-565df6858fb6"
      },
      "source": [
        "# load the network weights\r\n",
        "filename = \"/content/weights-improvement-30-0.9777.hdf5\"\r\n",
        "model.load_weights(filename)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\r\n",
        "# pick a random seed\r\n",
        "start = numpy.random.randint(0, len(dataX)-1)\r\n",
        "pattern = dataX[start]\r\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\r\n",
        "\r\n",
        "# generate characters\r\n",
        "for i in range(10):\r\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\r\n",
        "\tx = x / float(n_vocab)\r\n",
        "\tprediction = model.predict(x, verbose=0)\r\n",
        "\tindex = numpy.argmax(prediction)\r\n",
        "\tresult = int_to_char[index]\r\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\r\n",
        "\tsys.stdout.write(result)\r\n",
        "\tpattern.append(index)\r\n",
        "\tpattern = pattern[1:len(pattern)]\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" rî, le\n",
            "condizioni essenziali. la tutela delle libertà, la difesa delle forze\n",
            "economiche del paese, a \"\n",
            "l partito "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}