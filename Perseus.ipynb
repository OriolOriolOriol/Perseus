{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perseus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIJWhZifdv00rLi41O+BqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OriolOriolOriol/Perseus/blob/main/Perseus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhmCxZB4ePqd"
      },
      "source": [
        "import numpy\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw3cPnJIgI_U"
      },
      "source": [
        "Importato le librerie necessari per creare il modello.\r\n",
        "Ecco di seguito cosa fanno:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uubPzIo3gJPl"
      },
      "source": [
        "filename=\"/content/libro.txt\"\r\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\r\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y0hpbEBg8o1"
      },
      "source": [
        "Carichiamo  il libro  in memoria e convertire tutti i caratteri in minuscolo per ridurre il vocabolario che la rete deve imparare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wDlsNu2hClh",
        "outputId": "3e98c24f-3bb5-4dbc-bd70-ecd2c9819576"
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\r\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\r\n",
        "print(char_to_int)\r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, ';': 20, '?': 21, '_': 22, 'a': 23, 'b': 24, 'c': 25, 'd': 26, 'e': 27, 'f': 28, 'g': 29, 'h': 30, 'i': 31, 'j': 32, 'l': 33, 'm': 34, 'n': 35, 'o': 36, 'p': 37, 'q': 38, 'r': 39, 's': 40, 't': 41, 'u': 42, 'v': 43, 'x': 44, 'y': 45, 'z': 46, '«': 47, '°': 48, '²': 49, '¹': 50, '»': 51, 'à': 52, 'è': 53, 'ê': 54, 'ì': 55, 'î': 56, 'ò': 57, 'ù': 58}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEa4ImMChGQV"
      },
      "source": [
        "Ora che il libro è caricato, dobbiamo preparare i dati per la modellazione dalla rete neurale. Non possiamo modellare i caratteri direttamente, dobbiamo invece convertire i caratteri in numeri interi.\r\n",
        "Prendi ogni carattere e le ordini in abse al suo codice ASCII. Successivamente si mappa ogni singolo carattere con un numero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTMjQ8smikcC",
        "outputId": "8f0c0970-ed8f-48c7-c883-eebca710991c"
      },
      "source": [
        "n_chars = len(raw_text)\r\n",
        "n_vocab = len(chars)\r\n",
        "print (\"Total Characters: \", n_chars)\r\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  120448\n",
            "Total Vocab:  59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jteVqfsmi7hk"
      },
      "source": [
        "Ora che il libro è stato caricato e la mappatura preparata, possiamo riassumere il set di dati."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9as4lF8i78z",
        "outputId": "d9f07b61-dc94-42f3-b538-d4be391083a4"
      },
      "source": [
        "lunghezza_sequenza= 100\r\n",
        "dataX=[]\r\n",
        "dataY=[]\r\n",
        "for i in range(0, n_chars - lunghezza_sequenza, 1):\r\n",
        "  seq_in= raw_text[i:i + lunghezza_sequenza]\r\n",
        "  seq_output= raw_text[i + lunghezza_sequenza]\r\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\r\n",
        "  dataY.append(char_to_int[seq_output])\r\n",
        "n_patterns= len(dataX)\r\n",
        "print(\"Totali Patterns: \", n_patterns)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Totali Patterns:  120348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSLNaZ0zi-0Q"
      },
      "source": [
        "Ora dobbiamo definire i dati di addestramento per la rete. C'è molta flessibilità nel modo in cui scegli di suddividere il testo ed esporlo alla rete durante l'allenamento.\r\n",
        "\r\n",
        "Possiamo dividere il testo del libro in sottosequenze di 100 caratteri. Ciascun training patterns è composto da 100 fasi temporali di un carattere X seguito da un output di carattere Y.\r\n",
        "\r\n",
        "\r\n",
        "Quando creiamo queste sequenze, facciamo scorrere questa finestra lungo l'intero libro un carattere alla volta, consentendo a ciascun carattere di apprendere dai 100 caratteri che lo hanno preceduto (tranne i primi 100 caratteri ovviamente).\r\n",
        "\r\n",
        "Una volta suddivisi convertiamo i caratteri in numeri interi usando la funzione creata in precedenza.\r\n",
        "\r\n",
        "L'esecuzione del codice fino a questo punto ci mostra che quando suddividiamo il set di dati in dati di addestramento affinché la rete apprenda che abbiamo poco meno di 150.000 modelli di addestramento. Questo ha senso escludendo i primi 100 caratteri, abbiamo un modello di addestramento per prevedere ciascuno dei caratteri rimanenti.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nafe39T6pc8k"
      },
      "source": [
        "#X\r\n",
        "\r\n",
        "# reshape X to be [samples, time steps, features]\r\n",
        "X = numpy.reshape(dataX, (n_patterns, lunghezza_sequenza, 1))\r\n",
        "# normalize\r\n",
        "X = X / float(n_vocab)\r\n",
        "# reshape X to be [samples, time steps, features]\r\n",
        "X = numpy.reshape(dataX, (n_patterns, lunghezza_sequenza, 1))\r\n",
        "# normalize\r\n",
        "X = X / float(n_vocab)\r\n",
        "\r\n",
        "#Y\r\n",
        "# one hot encode the output variable\r\n",
        "y = np_utils.to_categorical(dataY)\r\n",
        "# one hot encode the output variable\r\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAPUnhEwpdIE"
      },
      "source": [
        "Ora che abbiamo preparato i nostri dati di allenamento, dobbiamo trasformarli in modo che siano adatti per l'uso con Keras.\r\n",
        "\r\n",
        "Per prima cosa dobbiamo trasformare l'elenco delle sequenze di input nella forma [campioni, fasi temporali, caratteristiche] attesi da una rete LSTM.\r\n",
        "\r\n",
        "Successivamente è necessario ridimensionare gli interi nell'intervallo da 0 a 1 per rendere i modelli più facili da apprendere dalla rete LSTM che utilizza la funzione di attivazione del sigmoide per impostazione predefinita.\r\n",
        "\r\n",
        "Infine, dobbiamo convertire i modelli di output (singoli caratteri convertiti in interi) in una codifica a caldo. Questo è così che possiamo configurare la rete per prevedere la probabilità di ciascuno dei 47 diversi caratteri nel vocabolario (una rappresentazione più semplice) piuttosto che cercare di costringerla a prevedere con precisione il carattere successivo. Ogni valore y viene convertito in un vettore sparse con una lunghezza di 47, pieno di zeri tranne con un 1 nella colonna per la lettera (numero intero) che il modello rappresenta.\r\n",
        "\r\n",
        "Ad esempio, quando \"n\" (valore intero 31) è una codifica a caldo, appare come segue:\r\n",
        "\r\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcNLQRRQqwxg",
        "outputId": "64e44e76-5433-49ec-c1bf-1cec0ad2e7f0"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\r\n",
        "model.add(Dense(y.shape[1], activation='softmax')) #Softmax output shape\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "# define the checkpoint\r\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "\r\n",
        "model.fit(X, y, epochs=30, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "941/941 [==============================] - 20s 13ms/step - loss: 3.0087\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.95358, saving model to weights-improvement-01-2.9536.hdf5\n",
            "Epoch 2/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.8340\n",
            "\n",
            "Epoch 00002: loss improved from 2.95358 to 2.80814, saving model to weights-improvement-02-2.8081.hdf5\n",
            "Epoch 3/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.7591\n",
            "\n",
            "Epoch 00003: loss improved from 2.80814 to 2.74885, saving model to weights-improvement-03-2.7488.hdf5\n",
            "Epoch 4/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.7211\n",
            "\n",
            "Epoch 00004: loss improved from 2.74885 to 2.71788, saving model to weights-improvement-04-2.7179.hdf5\n",
            "Epoch 5/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.6978\n",
            "\n",
            "Epoch 00005: loss improved from 2.71788 to 2.68626, saving model to weights-improvement-05-2.6863.hdf5\n",
            "Epoch 6/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.6575\n",
            "\n",
            "Epoch 00006: loss improved from 2.68626 to 2.65086, saving model to weights-improvement-06-2.6509.hdf5\n",
            "Epoch 7/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.6173\n",
            "\n",
            "Epoch 00007: loss improved from 2.65086 to 2.60808, saving model to weights-improvement-07-2.6081.hdf5\n",
            "Epoch 8/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.5702\n",
            "\n",
            "Epoch 00008: loss improved from 2.60808 to 2.56749, saving model to weights-improvement-08-2.5675.hdf5\n",
            "Epoch 9/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.5267\n",
            "\n",
            "Epoch 00009: loss improved from 2.56749 to 2.52548, saving model to weights-improvement-09-2.5255.hdf5\n",
            "Epoch 10/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.4874\n",
            "\n",
            "Epoch 00010: loss improved from 2.52548 to 2.48530, saving model to weights-improvement-10-2.4853.hdf5\n",
            "Epoch 11/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.4561\n",
            "\n",
            "Epoch 00011: loss improved from 2.48530 to 2.44796, saving model to weights-improvement-11-2.4480.hdf5\n",
            "Epoch 12/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.4116\n",
            "\n",
            "Epoch 00012: loss improved from 2.44796 to 2.41441, saving model to weights-improvement-12-2.4144.hdf5\n",
            "Epoch 13/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.3739\n",
            "\n",
            "Epoch 00013: loss improved from 2.41441 to 2.37850, saving model to weights-improvement-13-2.3785.hdf5\n",
            "Epoch 14/20\n",
            "941/941 [==============================] - 12s 13ms/step - loss: 2.3435\n",
            "\n",
            "Epoch 00014: loss improved from 2.37850 to 2.34541, saving model to weights-improvement-14-2.3454.hdf5\n",
            "Epoch 15/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.3156\n",
            "\n",
            "Epoch 00015: loss improved from 2.34541 to 2.31284, saving model to weights-improvement-15-2.3128.hdf5\n",
            "Epoch 16/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.2763\n",
            "\n",
            "Epoch 00016: loss improved from 2.31284 to 2.28254, saving model to weights-improvement-16-2.2825.hdf5\n",
            "Epoch 17/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.2499\n",
            "\n",
            "Epoch 00017: loss improved from 2.28254 to 2.25340, saving model to weights-improvement-17-2.2534.hdf5\n",
            "Epoch 18/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.2173\n",
            "\n",
            "Epoch 00018: loss improved from 2.25340 to 2.22315, saving model to weights-improvement-18-2.2232.hdf5\n",
            "Epoch 19/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.1892\n",
            "\n",
            "Epoch 00019: loss improved from 2.22315 to 2.19694, saving model to weights-improvement-19-2.1969.hdf5\n",
            "Epoch 20/20\n",
            "941/941 [==============================] - 13s 13ms/step - loss: 2.1679\n",
            "\n",
            "Epoch 00020: loss improved from 2.19694 to 2.16937, saving model to weights-improvement-20-2.1694.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f36d02020f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHgFV3E7qxDg"
      },
      "source": [
        "Qui definiamo un singolo livello LSTM nascosto con 256 unità di memoria. La rete utilizza dropout con una probabilità di 20. Il livello di output è un livello Denso che utilizza la funzione di attivazione softmax per produrre una previsione di probabilità per ciascuno dei 47 caratteri compresi tra 0 e 1.\r\n",
        "\r\n",
        "Il problema è in realtà un problema di classificazione di un singolo carattere con 47 classi e come tale è definito come l'ottimizzazione della perdita di log (entropia incrociata), qui utilizzando l'algoritmo di ottimizzazione ADAM per la velocità.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Non siamo interessati al modello più accurato (accuratezza della classificazione) del set di dati di addestramento. Questo sarebbe un modello che prevede perfettamente ogni personaggio nel set di dati di addestramento. Ci interessa invece una generalizzazione del dataset che minimizzi la funzione di perdita scelta. Cerchiamo un equilibrio tra generalizzazione e overfitting ma a corto di memorizzazione.\r\n",
        "\r\n",
        "\r\n",
        "La rete è lenta da addestrare (circa 300 secondi per epoca su una GPU Nvidia K520). A causa della lentezza e dei nostri requisiti di ottimizzazione, utilizzeremo il checkpoint del modello per registrare tutti i pesi della rete da archiviare ogni volta che si osserva un miglioramento della perdita alla fine dell'epoca. Useremo il miglior set di pesi (perdita minima) per istanziare il nostro modello generativo nella sezione successiva."
      ]
    }
  ]
}